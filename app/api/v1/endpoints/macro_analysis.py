"""
Endpoints para An√°lise Macroecon√¥mica Avan√ßada
Sistema completo de an√°lise macro com dados hist√≥ricos
"""

import asyncio
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from fastapi import APIRouter, Depends, HTTPException, Query, BackgroundTasks
from sqlalchemy.orm import Session

from app.core.database import get_db
from app.services.macro_analysis.pipeline import MacroAnalysisPipeline, PipelineConfig
from app.services.macro_analysis.historical_data_manager import HistoricalDataManager

logger = logging.getLogger(__name__)
router = APIRouter()


@router.get("/analysis/complete")
async def get_complete_macro_analysis(
    country: str = Query("BRA", description="Pa√≠s para an√°lise"),
    force_refresh: bool = Query(False, description="For√ßar atualiza√ß√£o dos dados"),
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Executa an√°lise macroecon√¥mica completa
    
    Args:
        country: Pa√≠s para an√°lise (BRA, USA, GLOBAL)
        force_refresh: For√ßar atualiza√ß√£o dos dados
        db: Sess√£o do banco de dados
        
    Returns:
        Dict com resultado completo da an√°lise
    """
    try:
        logger.info(f"üöÄ Iniciando an√°lise macro completa para {country}")
        
        # Configurar pipeline
        config = PipelineConfig(
            country=country,
            force_data_refresh=force_refresh
        )
        
        # Executar an√°lise
        pipeline = MacroAnalysisPipeline(db, config)
        result = await pipeline.run_complete_analysis(country, force_refresh)
        
        # Converter resultado para dict
        analysis_dict = {
            'country': country,
            'regimes': result.regimes,
            'market_cycles': result.market_cycles,
            'transitions': result.transitions,
            'signals': result.signals,
            'confidence': result.confidence,
            'data_quality': result.data_quality,
            'timestamp': result.timestamp.isoformat(),
            'pipeline_metadata': getattr(result, 'pipeline_metadata', {})
        }
        
        logger.info(f"‚úÖ An√°lise macro conclu√≠da para {country} - Confian√ßa: {result.confidence:.2%}")
        return analysis_dict
        
    except Exception as e:
        logger.error(f"‚ùå Erro na an√°lise macro: {e}")
        raise HTTPException(status_code=500, detail=f"Erro na an√°lise macro: {str(e)}")


@router.get("/analysis/historical")
async def get_historical_analysis(
    country: str = Query("BRA", description="Pa√≠s para an√°lise"),
    start_date: str = Query(None, description="Data de in√≠cio (YYYY-MM-DD)"),
    end_date: str = Query(None, description="Data de fim (YYYY-MM-DD)"),
    months: int = Query(12, description="N√∫mero de meses para an√°lise"),
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Executa an√°lise hist√≥rica para per√≠odo espec√≠fico
    
    Args:
        country: Pa√≠s para an√°lise
        start_date: Data de in√≠cio
        end_date: Data de fim
        months: N√∫mero de meses (se start_date n√£o especificado)
        db: Sess√£o do banco de dados
        
    Returns:
        Dict com an√°lise hist√≥rica
    """
    try:
        logger.info(f"üìö Iniciando an√°lise hist√≥rica para {country}")
        
        # Processar datas
        if start_date:
            start_dt = datetime.fromisoformat(start_date)
        else:
            start_dt = datetime.now() - timedelta(days=months * 30)
        
        if end_date:
            end_dt = datetime.fromisoformat(end_date)
        else:
            end_dt = datetime.now()
        
        # Executar an√°lise hist√≥rica
        pipeline = MacroAnalysisPipeline(db)
        result = await pipeline.run_historical_analysis(country, start_dt, end_dt)
        
        logger.info(f"‚úÖ An√°lise hist√≥rica conclu√≠da para {country}")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erro na an√°lise hist√≥rica: {e}")
        raise HTTPException(status_code=500, detail=f"Erro na an√°lise hist√≥rica: {str(e)}")


@router.get("/analysis/comparative")
async def get_comparative_analysis(
    countries: str = Query("BRA,USA", description="Pa√≠ses para compara√ß√£o (separados por v√≠rgula)"),
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Executa an√°lise comparativa entre pa√≠ses
    
    Args:
        countries: Lista de pa√≠ses separados por v√≠rgula
        db: Sess√£o do banco de dados
        
    Returns:
        Dict com an√°lise comparativa
    """
    try:
        country_list = [c.strip().upper() for c in countries.split(',')]
        logger.info(f"üåç Iniciando an√°lise comparativa para {country_list}")
        
        # Executar an√°lise comparativa
        pipeline = MacroAnalysisPipeline(db)
        result = await pipeline.run_comparative_analysis(country_list)
        
        logger.info(f"‚úÖ An√°lise comparativa conclu√≠da para {len(country_list)} pa√≠ses")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erro na an√°lise comparativa: {e}")
        raise HTTPException(status_code=500, detail=f"Erro na an√°lise comparativa: {str(e)}")


@router.get("/analysis/sentiment")
async def get_sentiment_analysis(
    force_refresh: bool = Query(False, description="For√ßar atualiza√ß√£o dos dados"),
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Executa an√°lise de sentimento de mercado
    
    Args:
        force_refresh: For√ßar atualiza√ß√£o dos dados
        db: Sess√£o do banco de dados
        
    Returns:
        Dict com an√°lise de sentimento
    """
    try:
        logger.info("üòä Iniciando an√°lise de sentimento")
        
        # Executar an√°lise de sentimento
        pipeline = MacroAnalysisPipeline(db)
        result = await pipeline.run_sentiment_analysis(force_refresh)
        
        logger.info("‚úÖ An√°lise de sentimento conclu√≠da")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erro na an√°lise de sentimento: {e}")
        raise HTTPException(status_code=500, detail=f"Erro na an√°lise de sentimento: {str(e)}")


@router.get("/analysis/risk-assessment")
async def get_risk_assessment(
    country: str = Query("BRA", description="Pa√≠s para avalia√ß√£o"),
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Executa avalia√ß√£o de risco macroecon√¥mico
    
    Args:
        country: Pa√≠s para avalia√ß√£o
        db: Sess√£o do banco de dados
        
    Returns:
        Dict com avalia√ß√£o de risco
    """
    try:
        logger.info(f"‚ö†Ô∏è Iniciando avalia√ß√£o de risco para {country}")
        
        # Executar avalia√ß√£o de risco
        pipeline = MacroAnalysisPipeline(db)
        result = await pipeline.run_risk_assessment(country)
        
        logger.info(f"‚úÖ Avalia√ß√£o de risco conclu√≠da para {country}")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erro na avalia√ß√£o de risco: {e}")
        raise HTTPException(status_code=500, detail=f"Erro na avalia√ß√£o de risco: {str(e)}")


@router.post("/data/sync")
async def sync_historical_data(
    background_tasks: BackgroundTasks,
    series_list: Optional[List[str]] = Query(None, description="Lista de s√©ries para sincronizar"),
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Sincroniza dados hist√≥ricos
    
    Args:
        background_tasks: Tarefas em background
        series_list: Lista de s√©ries para sincronizar
        db: Sess√£o do banco de dados
        
    Returns:
        Dict com resultado da sincroniza√ß√£o
    """
    try:
        logger.info("üîÑ Iniciando sincroniza√ß√£o de dados hist√≥ricos")
        
        # Executar sincroniza√ß√£o
        historical_manager = HistoricalDataManager(db)
        result = await historical_manager.sync_all_series(series_list)
        
        logger.info(f"‚úÖ Sincroniza√ß√£o conclu√≠da: {result['successful']} sucessos, {result['failed']} falhas")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erro na sincroniza√ß√£o: {e}")
        raise HTTPException(status_code=500, detail=f"Erro na sincroniza√ß√£o: {str(e)}")


@router.get("/data/summary")
async def get_data_summary(
    series_name: Optional[str] = Query(None, description="Nome da s√©rie espec√≠fica"),
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Obt√©m resumo dos dados hist√≥ricos
    
    Args:
        series_name: Nome da s√©rie espec√≠fica (opcional)
        db: Sess√£o do banco de dados
        
    Returns:
        Dict com resumo dos dados
    """
    try:
        logger.info("üìä Gerando resumo dos dados hist√≥ricos")
        
        historical_manager = HistoricalDataManager(db)
        
        if series_name:
            # Resumo de s√©rie espec√≠fica
            result = await historical_manager.get_data_summary(series_name)
        else:
            # Resumo geral do banco
            result = await historical_manager.get_database_stats()
        
        logger.info("‚úÖ Resumo dos dados gerado")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao gerar resumo: {e}")
        raise HTTPException(status_code=500, detail=f"Erro ao gerar resumo: {str(e)}")


@router.get("/data/historical/{series_name}")
async def get_historical_series_data(
    series_name: str,
    start_date: Optional[str] = Query(None, description="Data de in√≠cio (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="Data de fim (YYYY-MM-DD)"),
    months: Optional[int] = Query(None, description="N√∫mero de meses"),
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Obt√©m dados hist√≥ricos de uma s√©rie espec√≠fica
    
    Args:
        series_name: Nome da s√©rie
        start_date: Data de in√≠cio
        end_date: Data de fim
        months: N√∫mero de meses
        db: Sess√£o do banco de dados
        
    Returns:
        Dict com dados da s√©rie
    """
    try:
        logger.info(f"üìà Buscando dados hist√≥ricos para {series_name}")
        
        # Processar datas
        start_dt = None
        end_dt = None
        
        if start_date:
            start_dt = datetime.fromisoformat(start_date)
        if end_date:
            end_dt = datetime.fromisoformat(end_date)
        
        # Buscar dados
        historical_manager = HistoricalDataManager(db)
        data = await historical_manager.get_historical_data(
            series_name=series_name,
            start_date=start_dt,
            end_date=end_dt,
            months=months
        )
        
        if data.empty:
            return {
                'series_name': series_name,
                'data': [],
                'message': 'Nenhum dado encontrado',
                'count': 0
            }
        
        # Converter para formato JSON
        data_dict = data.to_dict('records')
        
        logger.info(f"‚úÖ {len(data_dict)} registros encontrados para {series_name}")
        return {
            'series_name': series_name,
            'data': data_dict,
            'count': len(data_dict),
            'date_range': {
                'start': data['date'].min().isoformat(),
                'end': data['date'].max().isoformat()
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao buscar dados hist√≥ricos: {e}")
        raise HTTPException(status_code=500, detail=f"Erro ao buscar dados hist√≥ricos: {str(e)}")


@router.delete("/data/cleanup")
async def cleanup_old_data(
    days_to_keep: int = Query(365, description="Dias de dados para manter"),
    db: Session = Depends(get_db)
) -> Dict[str, Any]:
    """
    Remove dados antigos do banco (manuten√ß√£o)
    
    Args:
        days_to_keep: N√∫mero de dias de dados para manter
        db: Sess√£o do banco de dados
        
    Returns:
        Dict com resultado da limpeza
    """
    try:
        logger.info(f"üßπ Iniciando limpeza de dados antigos (manter {days_to_keep} dias)")
        
        # Executar limpeza
        historical_manager = HistoricalDataManager(db)
        result = await historical_manager.cleanup_old_data(days_to_keep)
        
        logger.info(f"‚úÖ Limpeza conclu√≠da: {result['records_removed']} registros removidos")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erro na limpeza: {e}")
        raise HTTPException(status_code=500, detail=f"Erro na limpeza: {str(e)}")


@router.get("/health")
async def get_macro_analysis_health(db: Session = Depends(get_db)) -> Dict[str, Any]:
    """
    Health check para an√°lise macro
    
    Args:
        db: Sess√£o do banco de dados
        
    Returns:
        Dict com status de sa√∫de
    """
    try:
        # Verificar componentes
        historical_manager = HistoricalDataManager(db)
        pipeline = MacroAnalysisPipeline(db)
        
        # Testar conectividade
        db_stats = await historical_manager.get_database_stats()
        
        return {
            'status': 'healthy',
            'database': 'connected',
            'total_records': db_stats.get('total_records', 0),
            'total_series': db_stats.get('total_series', 0),
            'pipeline': 'ready',
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erro no health check: {e}")
        return {
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }
